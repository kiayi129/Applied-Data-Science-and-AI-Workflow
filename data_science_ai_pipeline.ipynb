{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75wjsHm82Hhe"
      },
      "source": [
        "# Part 1: Machine Learning Workflow and Ethics\n",
        "*   Step 1: Data Preparation\n",
        "*   Step 2: Data Visualization\n",
        "*   Step 3: Model Evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiLQ6oADUK-"
      },
      "source": [
        "# 1.0 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkvUV2E1Fbj"
      },
      "source": [
        "# 1.1 Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEFBNmHc_0xp"
      },
      "outputs": [],
      "source": [
        "# 1.1 DATA IMPORT\n",
        "!pip install datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading California Housing dataset...\")\n",
        "california_housing = load_dataset(\"leostelon/california-housing\")\n",
        "housing_data = pd.DataFrame(california_housing['train'])  # Convert to Pandas DataFrame\n",
        "print(\"California Housing dataset loaded successfully.\")\n",
        "\n",
        "# Display dataset\n",
        "print(housing_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyInqB5D1GB9"
      },
      "source": [
        "# 1.2 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uAXuRlo165A"
      },
      "outputs": [],
      "source": [
        "# 1.2 DATA EXPLORATION (NOT THAT INPORTANT, JUST SHOWING DATA USING DIFFERENT METHOD)\n",
        "# Structure of data\n",
        "print(\"****** Structure of Data ******\")\n",
        "print(housing_data.info())\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\\n****** Summary Statistics ******\")\n",
        "print(housing_data.describe())\n",
        "\n",
        "# Dimensions of the dataset\n",
        "print(\"\\n\\n****** Dataset Dimensions ******\")\n",
        "print(f'Dataset dimensions: {housing_data.shape}')\n",
        "\n",
        "# Column names\n",
        "print(\"\\n\\n****** Column Names ******\")\n",
        "print(f'Column names: {housing_data.columns.tolist()}')\n",
        "\n",
        "# First few and last few rows\n",
        "print(\"\\n\\n****** First Few Rows ******\")\n",
        "print(housing_data.head())\n",
        "print(\"\\n\\n****** Last Few Rows ******\")\n",
        "print(housing_data.tail())\n",
        "\n",
        "# Check unique values in categorical column\n",
        "print(\"\\n\\n****** Unique Values in Categorical Column (ocean_proximity) ******\")\n",
        "print(housing_data['ocean_proximity'].value_counts())\n",
        "\n",
        "# Display missing values count\n",
        "print(\"\\n\\n****** Missing Values in Dataset ******\")\n",
        "missing_values = housing_data.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sapybcWg1GRH"
      },
      "source": [
        "# 1.3 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy4q6BkB1eFs"
      },
      "outputs": [],
      "source": [
        "# 1.3 DATA CLEANING\n",
        "# Check for missing values\n",
        "print(\"****** Checking for Missing Values ******\")\n",
        "missing_values = housing_data.isnull().sum()\n",
        "print(f'Missing Values per Column:\\n{missing_values}')\n",
        "\n",
        "# Drop rows with missing values (instead of filling with median/mean)\n",
        "print(\"\\n****** Removing Rows with Missing Values ******\")\n",
        "housing_data = housing_data.dropna()\n",
        "print(f'Dataset dimensions after removing missing values: {housing_data.shape}')\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\n****** Checking for Duplicate Rows ******\")\n",
        "duplicates = housing_data.duplicated().sum()\n",
        "print(f'Number of duplicate rows: {duplicates}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSDpDkgd1GeV"
      },
      "source": [
        "# 1.4 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWZQvJom1yn0"
      },
      "outputs": [],
      "source": [
        "# 1.4 DATA PRE-PROCESSING\n",
        "# Encode categorical variable (one-hot encoding)\n",
        "housing_data.loc[:, 'ocean_proximity_original'] = housing_data['ocean_proximity']\n",
        "housing_data = pd.get_dummies(housing_data, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "# Correlation Heatmap (Check for irrelevant data)\n",
        "plt.figure(figsize=(10, 8))\n",
        "numeric_housing_data = housing_data.select_dtypes(include=['number'])\n",
        "sns.heatmap(numeric_housing_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap of Housing Features\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Engineering (New features generated)\n",
        "housing_data['avg_household_size'] = housing_data['households'] / housing_data['population']\n",
        "housing_data['price_per_capita'] = housing_data['median_house_value'] / housing_data['population']\n",
        "\n",
        "# Feature Selection [ Drop columns with high correlation (>0.9) ]\n",
        "housing_data.drop(columns=[\"total_rooms\", \"total_bedrooms\"], inplace=True)\n",
        "print(\"\\n****** Removing Columns with High Correlation ******\")\n",
        "\n",
        "# Display cleaned and preprocessed data\n",
        "print(housing_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBvqT-Lf97jk"
      },
      "source": [
        "# 2.0 Data Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcjNwUi9-Nk9"
      },
      "source": [
        "# 2.1 Distribution of House Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lMDGin2AyaN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(housing_data['median_house_value'], bins=50, kde=True, color='blue')\n",
        "plt.xlabel('Median House Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of House Prices')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpigRXpmBAMa"
      },
      "source": [
        "# 2.2 House Value vs. Median Income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8LlhHWgEC_P"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=housing_data['median_income'], y=housing_data['median_house_value'], alpha=0.5)\n",
        "plt.xlabel(\"Median Income\")\n",
        "plt.ylabel(\"Median House Value\")\n",
        "plt.title(\"House Value vs. Median Income\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpMD0SPBAnH"
      },
      "source": [
        "# 2.3 Population vs. Median House Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpfcNRnsFg2A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=housing_data['population'], y=housing_data['median_house_value'], alpha=0.3)\n",
        "plt.xlabel(\"Population\")\n",
        "plt.ylabel(\"Median House Value\")\n",
        "plt.title(\"House Value vs. Population\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulq2M17LBA0B"
      },
      "source": [
        "# 2.4 Number of Houses by Ocean Proximity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gin3Qjb6Fn62"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=housing_data['ocean_proximity_original'], palette='Set2')\n",
        "plt.xlabel(\"Ocean Proximity\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Number of Houses by Ocean Proximity\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5wkIHTdBBAi"
      },
      "source": [
        "# 2.5 Housing Density (Households vs. Population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7UmryZHF4Xz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=housing_data['households'], y=housing_data['population'], alpha=0.5)\n",
        "plt.xlabel(\"Number of Households\")\n",
        "plt.ylabel(\"Population\")\n",
        "plt.title(\"Households vs. Population\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwaWk7fiBBRZ"
      },
      "source": [
        "# 2.6 Distribution of Housing Median Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZncmz51BU9X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(housing_data['housing_median_age'], bins=30, kde=True, color='purple')\n",
        "plt.xlabel(\"Housing Median Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Housing Age\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUXhpPdoCtR0"
      },
      "source": [
        "# 3.0 Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr1KszUDYTs"
      },
      "source": [
        "# 3.1 Preprocess before Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RYqVbC9DgCV"
      },
      "outputs": [],
      "source": [
        "# Scale numerical features (SCALING MAKES MACHINE LEARNING MODEL PERFORMS BETTER)\n",
        "# FOR EXAMPLE : Linear Regression, Logistic Regression, Support Vector Machines, K-Nearest Neighbors, K-Means Clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_features = housing_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "scaler = StandardScaler()\n",
        "housing_data[numerical_features] = scaler.fit_transform(housing_data[numerical_features])\n",
        "print(housing_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WskW35rhXI79"
      },
      "source": [
        "# 3.2 Supervised Learning (Linear Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2v1vPcfTTic"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "housing_data = pd.get_dummies(housing_data, columns=['ocean_proximity_original'], drop_first=True)\n",
        "\n",
        "# Define features and target\n",
        "X = housing_data.drop(columns=['median_house_value'])  # Features\n",
        "y = housing_data['median_house_value']  # Target variable\n",
        "\n",
        "# Split data into training and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Regression Performance:\\nR-squared: {r2:.2f}\\nMAE: {mae:.2f}\\nMSE: {mse:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NshNgNwOXciI"
      },
      "source": [
        "# 3.3 Supervised Learning (Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhZGP0YAXh4e"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Train the Decision Tree model\n",
        "dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "print(f\"Decision Tree Performance:\\nR-squared: {r2_dt:.2f}\\nMAE: {mae_dt:.2f}\\nMSE: {mse_dt:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32MeW661XlIl"
      },
      "source": [
        "# 3.4 Unsupervised Learning (K-Means Clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvjSVryxYSlh"
      },
      "source": [
        "# 4.0 Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIo1Bba9XooK"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Choose number of clusters (elbow method can be used to find optimal k)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "housing_data['cluster'] = kmeans.fit_predict(X)\n",
        "print(\"K-Means clustering applied. Cluster labels added to dataset.\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=housing_data['cluster'], palette='viridis')\n",
        "plt.title(\"K-Means Clustering of Housing Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smntLlR3XxZI"
      },
      "source": [
        "# 5.0 Model Interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjnvbiYAX9JL"
      },
      "source": [
        "# 5.1 Regression Model Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZnifO8X7dX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compare actual vs predicted values for linear regression\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)\n",
        "plt.xlabel(\"Actual Median House Value\")\n",
        "plt.ylabel(\"Predicted Median House Value\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj4ILXhnYFqn"
      },
      "source": [
        "# 5.2 Decision Tree Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgKFO1pJYIBw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get feature importance from Decision Tree\n",
        "feature_importance = pd.Series(dt_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=feature_importance, y=feature_importance.index)\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance from Decision Tree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6wqIlCsxEzt"
      },
      "source": [
        "# Part 2: Natural Language Processing and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy21ounuPqBO"
      },
      "source": [
        "# 6.0 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db6iVX4_P9tz"
      },
      "source": [
        "# 6.1 Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdrXVlK9QA-7"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading Amazon Polarity dataset...\")\n",
        "# Load the Amazon Polarity dataset\n",
        "amazon_polarity = load_dataset(\"fancyzhx/amazon_polarity\")\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "amazon_data = pd.DataFrame(amazon_polarity['train'])  # Training data\n",
        "print(\"Amazon Polarity dataset loaded successfully.\")\n",
        "\n",
        "# Extract a sample of 100000 rows from the dataset\n",
        "amazon_data = amazon_data.head(100000)\n",
        "\n",
        "# Display first 5 rows of the dataset\n",
        "print(amazon_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM0KfXzUUQ-J"
      },
      "source": [
        "# 6.2 Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nRG3lyFR24R"
      },
      "outputs": [],
      "source": [
        "# # # Uninstall conflicting packages\n",
        "!pip uninstall -y numpy scipy gensim tensorflow keras\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install numpy==1.26.4\n",
        "!pip install scipy==1.13.1\n",
        "!pip install gensim==4.3.3\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
        "    tokens = text.split()  # Tokenize text\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply text cleaning to the 'content' column\n",
        "amazon_data['cleaned_content'] = amazon_data['content'].apply(clean_text)\n",
        "\n",
        "# Display the first 5 rows after preprocessing\n",
        "print(amazon_data[['content', 'cleaned_content']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hOypG47WzWy"
      },
      "source": [
        "# 6.3 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j68qwqcXDiJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenize the cleaned content\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')  # Limit vocab size to 10,000\n",
        "tokenizer.fit_on_texts(amazon_data['cleaned_content'])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(amazon_data['cleaned_content'])\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "max_length = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Get the labels\n",
        "labels = amazon_data['label'].values\n",
        "\n",
        "print(f\"Tokenized and padded sequences shape: {padded_sequences.shape}\")\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZSjD4jnXEG1"
      },
      "source": [
        "# 6.4 Pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sII_fiXBXNO0"
      },
      "outputs": [],
      "source": [
        "# Used to divide the words into different vector dimensions\n",
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "print(\"GloVe embeddings downloaded and extracted successfully!\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "glove_path = \"glove.6B.100d.txt\"  # Make sure to download GloVe embeddings before running\n",
        "embedding_index = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "print(f\"Loaded {len(embedding_index)} word vectors.\")\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(10000, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < num_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"Embedding matrix created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYo9jaTVXMgb"
      },
      "source": [
        "# 7.0 Deep Learning Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx_neZ2vXU6-"
      },
      "source": [
        "# 7.1 LSTM Model (Long Short-Term Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bu5iTI2UXduy",
        "outputId": "4de24c9e-dfaa-4912-a60e-57aede92880b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_words' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-470776864.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 64 LSTM units, return_sequences=False for final output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dropout to prevent overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_words' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(LSTM(64, return_sequences=False))  # 64 LSTM units, return_sequences=False for final output\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification (sigmoid for probability)\n",
        "\n",
        "# Compute class weights from training data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.build(input_shape=(None, 100))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,             # Training data\n",
        "    y_train,             # Training data\n",
        "    validation_split=0.2,# Training labels\n",
        "    epochs=10,           # Number of training epochs\n",
        "    batch_size=32,       # Batch size for training\n",
        "    validation_data=(X_test, y_test), # Data for validation\n",
        "    class_weight=class_weight_dict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzb2MbmQXeKk"
      },
      "source": [
        "# 8.0 Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zLBUcndgXhLp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict on training data\n",
        "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions, labels=[0, 1])\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
